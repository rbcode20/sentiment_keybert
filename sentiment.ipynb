{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import keybert\n",
    "from keybert import KeyBERT\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'tweet_esp.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tdf = pd.read_json(datafile)\n",
    "tweets = tdf.transpose()\n",
    "tweets.info()\n",
    "tweets = tweets.assign(Time=pd.to_datetime(tweets.Datetime))\n",
    "\n",
    "print(\"Number of tweets: \",len(tweets['Text']))\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING\n",
    "\n",
    "tweet=[]\n",
    "d=[]\n",
    "\n",
    "\n",
    "CONTRACTIONS = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
    "                \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he had\", \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\", \"i'd\": \"i had\", \"i'll\" : \"i will\", \"i'm\" : \"i am\", \"i've\" : \"i have\", \"isn't\" : \"is not\",\"it'd\" : \"it had\",\n",
    "                \"it'll\" : \"it will\",\"it've\" : \"it have\",\"it's\" : \"it is \",\"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\", \"she'd\" : \"she had\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\",\n",
    "                \"there's\" : \"there is\", \"they'd\" : \"they had\",\"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we had\",\n",
    "                \"we're\" : \"we are\", \"we've\" :  \"we have\", \"weren't\" : \"were not\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \n",
    "                \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who had\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \n",
    "                \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\": \"would not\",\"you'd\" : \"you had\",\"you'll\" : \"you will\", \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\"}\n",
    "\n",
    "\n",
    "'''\n",
    "emoticons = {':-)' : \"happy\", ':)': \"happy\", ';)': \"happy\", ':o)': \"happy\", ':]': \"happy\", ':3': \"happy\", ':c)': \"happy\", ':>': \"happy\", '=]': \"happy\",\n",
    "             '8)': \"happy\", '=)': \"happy\", ':}': \"happy\", ':^)': \"happy\", ':-D': \"laughing\", ':D': \"laughing\", '8-D': \"laughing\", '8D': \"laughing\", \n",
    "             'x-D': \"laughing\", 'xD': \"laughing\", 'X-D': \"laughing\", 'XD': \"laughing\", '=-D': \"laughing\", '=D': \"laughing\", 'xp': \"laughing\", \n",
    "             'XP': \"laughing\", ':-p': \"laughing\", ':p': \"laughing\", '=p': \"laughing\", ':-b': \"laughing\", ':L': \"sad\", ':-/': \"sad\", '>:/': \"angry\", \n",
    "             ':S': \"sad\", '>:[': \"angry\", ':@': \"sad\", ':-(': \"sad\", ':[': \"sad\", ':-||': \"sad\", '=L': \"sad\", ':<': \"sad\", ':-[': \"sad\", ':-<': \"sad\", \n",
    "             '=\\\\': \"sad\", '=/': \"sad\", '>:(': \"angry\", ':(': \"sad\", '>.<': \"sad\", \":'-(\": \"crying\", \":'(\": \"crying\", ':\\\\': \"crying\", ':-c': \"sad\", \n",
    "             ':c' : \"angry\", ':{' : \"angry\", '>:\\\\' : \"angry\", ';(' : \"angry\"}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(tweets['Text'])):\n",
    "   #Removing HTML tags      \n",
    "        d = BeautifulSoup(tweets['Text'][i]).get_text() \n",
    "\n",
    "\n",
    "   #Replacing contractions\n",
    "        d = d.replace(\"â€™\",\"'\") \n",
    "        words = d.split() \n",
    "        reformed = [CONTRACTIONS[word] if word in CONTRACTIONS \n",
    "                    else word for word in words]\n",
    "        d = \" \".join(reformed)\n",
    "        \n",
    "\n",
    "  #Removing hashtags\n",
    "        d = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", d).split()) \n",
    "\n",
    "\n",
    "  #Removing URLs\n",
    "        d = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", d).split()) \n",
    "\n",
    "\n",
    "  #Removing punctuations\n",
    "        d = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", d).split()) \n",
    "\n",
    "\n",
    "  #Removing Numbers\n",
    "        pattern = r'[0-9]'\n",
    "        d = ' '.join(re.sub(pattern, \" \", d).split()) \n",
    "\n",
    "\n",
    "  #Removing characters\n",
    "        d = ' '.join(re.sub(\"\\W+\", \" \", d).split()) \n",
    "\n",
    "\n",
    "  #avoiding case sensitive issue\n",
    "        d = d.lower() \n",
    "\n",
    "  \n",
    "        tweet.append(d)\n",
    "tweet[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel = KeyBERT(model=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "keywords = kmodel.extract_keywords(tweet, keyphrase_ngram_range=(1, 1))\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing keywords into a file\n",
    "data_keywords = pd.DataFrame(keywords)\n",
    "data_keywords.to_csv('keybert_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into list of tuples from list of list of tuples\n",
    "\n",
    "single_list = [item for t in keywords for item in t]\n",
    "print (single_list)\n",
    "\n",
    "\n",
    "#only saving str values\n",
    "\n",
    "final_list = [i[0] for i in single_list]\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lemmatization\n",
    "token = [lemmatizer.lemmatize(w) for w in final_list]\n",
    "\n",
    "# Removing stopwords from the tokenized_text\n",
    "processed_text = []\n",
    "\n",
    "for word in token:\n",
    "    if word not in stopwords.words('english'):\n",
    "        processed_text.append(word)\n",
    "            \n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "text1 = ' '.join(cat for cat in processed_text)\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "# Instantiate Word Cloud\n",
    "wc = WordCloud(width=2400,\n",
    "               height=1500,\n",
    "               min_font_size=10,\n",
    "               background_color='white')\n",
    "# Generate a word cloud\n",
    "plt.figure(figsize = (24, 6))\n",
    "keyword_wc = wc.generate(text1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(keyword_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = obj.polarity_scores(text1)\n",
    "print(sentiment_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
